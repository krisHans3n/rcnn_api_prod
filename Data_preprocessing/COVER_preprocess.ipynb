{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coverage Dataset Preprocessing\n",
    "<font size=3>Generate the coordinates of the bounding box from the Groundtruth mask.<br>\n",
    "    We obtained the coverage dataset [here](https://ieeexplore.ieee.org/document/7532339)<br>\n",
    "    In addition, we divide all images into training and test sets according to the ratio of Tampering factors.<br>\n",
    "    The number of training and test sets is the same as RGB-N.<br>\n",
    "    We provide text files for the training and test sets. See Readme.md for more details.<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import scipy.io as io\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounding_box(image,mask,row_data):\n",
    "    box_list=[]\n",
    "    gray = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "    ret,binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n",
    "    contours, hierarchy = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    print(\"Contours number：\", len(contours))\n",
    "    contours = sorted(contours, key=lambda i: len(i),reverse=True)\n",
    "    x, y, w, h = cv2.boundingRect(contours[0])\n",
    "    x1=x\n",
    "    y1=y\n",
    "    x2=x+w\n",
    "    y2=y+h\n",
    "    box_list.append(str(x1)+'_'+str(y1)+'_'+ str(x2)+'_'+str(y2))\n",
    "    return box_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 2\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 2\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 2\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "(341, 629, 3)\n",
      "(314, 579, 3)\n",
      "95\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "Contours number： 1\n",
      "\n",
      "\n",
      "=======================Done==============================!\n"
     ]
    }
   ],
   "source": [
    "data_path='../dataset/COVER_DATASET/'\n",
    "mat_path = data_path+'label/TFlabel.mat'    # Tampering factors.\n",
    "probe_path=data_path+'probe/'\n",
    "data = io.loadmat(mat_path)\n",
    "cls=data['TFlabel']\n",
    "\n",
    "if not os.path.exists(probe_path):\n",
    "  os.makedirs(probe_path)\n",
    "\n",
    "n=1  # Coverage data contains 100 images.\n",
    "while n<=100:\n",
    "    img_id=str(n)\n",
    "    mask = load_image(data_path+'mask/'+img_id+'forged.tif')\n",
    "    image = load_image(data_path+'image/'+img_id+'t.tif')\n",
    "    \n",
    "    #A few images are different in size from the mask and need to be processed separately.\n",
    "    if(mask.shape!=image.shape):   \n",
    "        print(mask.shape)\n",
    "        print(image.shape)\n",
    "        print(img_id)\n",
    "\n",
    "    box_list = bounding_box(image,mask, img_id)\n",
    "    # Start from 0\n",
    "    cls_id=int(cls[n-1])\n",
    "    for i in range(0,len(box_list)):\n",
    "         name_str='TP_'+img_id+'_'+box_list[i]+'_'+str(cls_id)\n",
    "         cv2.imwrite(probe_path+name_str.rstrip('_')+ \".png\", image)\n",
    "    n=n+1\n",
    "print('\\n\\n=======================Done==============================!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of images is 100\n",
      "len pic_name: 100\n",
      "len mani_type: 100 \n",
      "\n",
      "=======Split train and test set========\n",
      "train set number: 75\n",
      "test set number: 25\n",
      "=============Split over==============\n"
     ]
    }
   ],
   "source": [
    "data_dir = probe_path  # FIXME\n",
    "ext = 'TP*'\n",
    "cls = ['1', '2', '3', '4', '5', '6']  # Tampering factors.\n",
    "filenames = glob(os.path.join(data_dir, ext))\n",
    "print('The number of images is %d'%len(filenames))\n",
    "pic_name=[]\n",
    "mani_type=[]\n",
    "for file in filenames:\n",
    "    content = os.path.splitext(os.path.basename(file))[0].split(\"_\")\n",
    "    pic_name.append(os.path.splitext(os.path.basename(file))[0])\n",
    "    mani_type .append(content[-1])\n",
    "    \n",
    "print(\"len pic_name: %d\"%len(pic_name))\n",
    "print(\"len mani_type: %d \\n\"%len(mani_type))\n",
    "print('=======Split train and test set========')\n",
    "pic_name_train, pic_name_test, mani_type_train, mani_type_test = train_test_split(pic_name, mani_type, test_size=0.25, random_state=0)\n",
    "\n",
    "print(\"train set number: %d\"%len(pic_name_train))\n",
    "print(\"test set number: %d\"%len(pic_name_test))\n",
    "\n",
    "with open(data_path+'cover_train_single.txt', 'w') as f:\n",
    "    for pic in pic_name_train:\n",
    "        content = pic.split(\"_\")\n",
    "        if content[-1] in cls:\n",
    "            content2 = [str(i) for i in content[2:-1]]\n",
    "            content3=' '.join(content2)\n",
    "            f.write('%s %s %s\\n' % (pic,content3,'tamper'))\n",
    "\n",
    "\n",
    "\n",
    "with open(data_path+'cover_test_single.txt', 'w') as f:\n",
    "    for pic in pic_name_test:\n",
    "        content = pic.split(\"_\")\n",
    "        if content[-1] in cls:\n",
    "            content2 = [str(i) for i in content[2:-1]]\n",
    "            content3=' '.join(content2)\n",
    "            f.write('%s %s %s\\n' % (pic,content3,'tamper'))\n",
    "\n",
    "\n",
    "print('=============Split over==============')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "“tensor0.12”",
   "language": "python",
   "name": "tensor0.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
